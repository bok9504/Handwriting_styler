{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "import pickle as pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from common.utils import pad_seq, bytes_to_file, read_split_image, round_function\n",
    "from common.utils import shift_and_resize_image, normalize_image, centering_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_iter(examples, batch_size, augment, with_charid=False):\n",
    "    # the transpose ops requires deterministic\n",
    "    # batch size, thus comes the padding\n",
    "    padded = pad_seq(examples, batch_size)           ## 배치사이즈를 맞게 만들어주는 함수\n",
    "\n",
    "    ## 이미지의 테두리 일부분을 잘라서 세로로 합쳐서 저장해주는 함수\n",
    "    ##### 학습시킬 이미지(target과 source가 함께 이어 붙어있는 이미지) 입력\n",
    "    \n",
    "    def process(img):\n",
    "        img = bytes_to_file(img)                     ## 이미지를 byte 형식으로 변환해주는 함수\n",
    "        try:\n",
    "            img_A, img_B = read_split_image(img)     ## 학습할 이미지를 두가지 이미지로 반 나누어주는 함수\n",
    "            if augment:\n",
    "                # augment the image by:\n",
    "                # 1) enlarge the image\n",
    "                # 2) random crop the image back to its original size\n",
    "                # NOTE: image A and B needs to be in sync as how much\n",
    "                # to be shifted\n",
    "                w, h = img_A.shape                   ## 이미지의 크기 입력\n",
    "                multiplier = random.uniform(1.00, 1.20)  ## 두개 사이의 수 랜덤 리턴\n",
    "                # add an eps to prevent cropping issue\n",
    "                nw = int(multiplier * w) + 1\n",
    "                nh = int(multiplier * h) + 1\n",
    "                shift_x = int(np.ceil(np.random.uniform(0.01, nw - w)))   ## 또다시 랜덤값을 올림해서 int로 바꿈\n",
    "                shift_y = int(np.ceil(np.random.uniform(0.01, nh - h)))   ## 또다시 랜덤값을 올림해서 int로 바꿈\n",
    "                img_A = shift_and_resize_image(img_A, shift_x, shift_y, nw, nh) ### 이미지의 일부분을 잘라서 저장해주는 함수\n",
    "                img_B = shift_and_resize_image(img_B, shift_x, shift_y, nw, nh) ### 이미지의 일부분을 잘라서 저장해주는 함수\n",
    "            img_A = normalize_image(img_A)                           ### normalize 해는 함수 (0~1값)\n",
    "            img_A = img_A.reshape(1, len(img_A), len(img_A[0]))\n",
    "            img_B = normalize_image(img_B)\n",
    "            img_B = img_B.reshape(1, len(img_B), len(img_B[0]))\n",
    "            return np.concatenate([img_A, img_B], axis=0)           ### numpy 배열 세로로 합치기\n",
    "        finally:\n",
    "            img.close()\n",
    "            \n",
    "    ## 저장된 데이터에 따라(1.package에 with_charid 여부) 배치사이즈로 자른 이후 yield로 리던하는 함수\n",
    "    ### 이미지를 배치사이즈에 맞게 잘라준다음 각각의 첫번째, 두번째, 세번째 원소를 따로 저장(각각 labels, charid, img_bytes로 추측)\n",
    "    ### with_charid의 True False 여부에 따른 다른 yield 사용\n",
    "    \n",
    "    def batch_iter(with_charid=with_charid):\n",
    "        for i in range(0, len(padded), batch_size):              ###  len(padded)를 batch_size에 맞게 i에 넣어주는 함수\n",
    "            batch = padded[i: i + batch_size]                     ###  배치 사이즈에 맞게 나누어줌\n",
    "            labels = [e[0] for e in batch]                       ###  각각 배치의 첫번째 원소만 모아서 넣음\n",
    "            if with_charid:\n",
    "                charid = [e[1] for e in batch]                   ###  각각 배치의 두번째 원소만 모아서 넣음\n",
    "                image = [process(e[2]) for e in batch]           ###  각각 배치의 세번째 원소만 모아서 넣음\n",
    "                image = np.array(image).astype(np.float32)\n",
    "                image = torch.from_numpy(image)                   ###  배열를 텐서 자료형으로 변환\n",
    "                # stack into tensor\n",
    "                yield [labels, charid, image]\n",
    "            else:\n",
    "                image = [process(e[1]) for e in batch]\n",
    "                image = np.array(image).astype(np.float32)\n",
    "                image = torch.from_numpy(image)\n",
    "                # stack into tensor\n",
    "                yield [labels, image]\n",
    "\n",
    "    return batch_iter(with_charid=with_charid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PickledImageProvider(object):\n",
    "    def __init__(self, obj_path, verbose):\n",
    "        self.obj_path = obj_path\n",
    "        self.verbose = verbose\n",
    "        self.examples = self.load_pickled_examples()\n",
    "\n",
    "    def load_pickled_examples(self):\n",
    "        with open(self.obj_path, \"rb\") as of:\n",
    "            examples = list()\n",
    "            while True:\n",
    "                try:\n",
    "                    e = pickle.load(of)\n",
    "                    examples.append(e)\n",
    "                except EOFError:\n",
    "                    break\n",
    "                except Exception:\n",
    "                    pass\n",
    "            if self.verbose:\n",
    "                print(\"unpickled total %d examples\" % len(examples))\n",
    "            return examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataProvider(object):\n",
    "    def __init__(self, data_dir, train_name=\"train.obj\", val_name=\"val.obj\", \\\n",
    "                 filter_by_font=None, filter_by_charid=None, verbose=True, val=True):\n",
    "        self.data_dir = data_dir\n",
    "        self.filter_by_font = filter_by_font\n",
    "        self.filter_by_charid = filter_by_charid\n",
    "        self.train_path = os.path.join(self.data_dir, train_name)\n",
    "        self.val_path = os.path.join(self.data_dir, val_name)\n",
    "        self.train = PickledImageProvider(self.train_path, verbose)\n",
    "        if val:\n",
    "            self.val = PickledImageProvider(self.val_path, verbose)\n",
    "        if self.filter_by_font:\n",
    "            if verbose:\n",
    "                print(\"filter by label ->\", filter_by_font)\n",
    "            self.train.examples = [e for e in self.train.examples if e[0] in self.filter_by_font]\n",
    "            if val:\n",
    "                self.val.examples = [e for e in self.val.examples if e[0] in self.filter_by_font]\n",
    "        if self.filter_by_charid:\n",
    "            if verbose:\n",
    "                print(\"filter by char ->\", filter_by_charid)\n",
    "            self.train.examples = [e for e in self.train.examples if e[1] in filter_by_charid]\n",
    "            if val:\n",
    "                self.val.examples = [e for e in self.val.examples if e[1] in filter_by_charid]\n",
    "        if verbose:\n",
    "            if val:\n",
    "                print(\"train examples -> %d, val examples -> %d\" % (len(self.train.examples), len(self.val.examples)))\n",
    "            else:\n",
    "                print(\"train examples -> %d\" % (len(self.train.examples)))\n",
    "\n",
    "                \n",
    "    def get_train_iter(self, batch_size, shuffle=True, with_charid=False):\n",
    "        training_examples = self.train.examples[:]\n",
    "        if shuffle:\n",
    "            np.random.shuffle(training_examples)\n",
    "           \n",
    "        if with_charid:\n",
    "            return get_batch_iter(training_examples, batch_size, augment=True, with_charid=True)\n",
    "        else:\n",
    "            return get_batch_iter(training_examples, batch_size, augment=True)\n",
    "\n",
    "        \n",
    "    def get_val_iter(self, batch_size, shuffle=True, with_charid=False):\n",
    "        \"\"\"\n",
    "        Validation iterator runs forever\n",
    "        \"\"\"\n",
    "        val_examples = self.val.examples[:]\n",
    "        if shuffle:\n",
    "            np.random.shuffle(val_examples)\n",
    "        if with_charid:\n",
    "            return get_batch_iter(val_examples, batch_size, augment=True, with_charid=True)\n",
    "        else:\n",
    "            return get_batch_iter(val_examples, batch_size, augment=True)\n",
    "\n",
    "        \n",
    "    def compute_total_batch_num(self, batch_size):\n",
    "        \"\"\"Total padded batch num\"\"\"\n",
    "        return int(np.ceil(len(self.train.examples) / float(batch_size)))\n",
    "\n",
    "    \n",
    "    def get_all_labels(self):\n",
    "        \"\"\"Get all training labels\"\"\"\n",
    "        return list({e[0] for e in self.train.examples})\n",
    "\n",
    "    \n",
    "    def get_train_val_path(self):\n",
    "        return self.train_path, self.val_path\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fixed_sample(sample_size, img_size, data_dir, save_dir, \\\n",
    "                      val=False, verbose=True, with_charid=True, resize_fix=90):\n",
    "    data_provider = TrainDataProvider(data_dir, verbose=verbose, val=val)\n",
    "    if not val:\n",
    "        train_batch_iter = data_provider.get_train_iter(sample_size, with_charid=with_charid)\n",
    "    else:\n",
    "        train_batch_iter = data_provider.get_val_iter(sample_size, with_charid=with_charid)\n",
    "        \n",
    "    for batch in train_batch_iter:\n",
    "        if with_charid:\n",
    "            font_ids, _, batch_images = batch\n",
    "        else:\n",
    "            font_ids, batch_images = batch\n",
    "        fixed_batch = batch_images.cuda()\n",
    "        fixed_source = fixed_batch[:, 1, :, :].reshape(sample_size, 1, img_size, img_size)\n",
    "        fixed_target = fixed_batch[:, 0, :, :].reshape(sample_size, 1, img_size, img_size)\n",
    "\n",
    "        # centering\n",
    "        for idx, (image_S, image_T) in enumerate(zip(fixed_source, fixed_target)):\n",
    "            image_S = image_S.cpu().detach().numpy().reshape(img_size, img_size)\n",
    "            image_S = np.array(list(map(round_function, image_S.flatten()))).reshape(128, 128)\n",
    "            image_S = centering_image(image_S, resize_fix=90)\n",
    "            fixed_source[idx] = torch.tensor(image_S).view([1, img_size, img_size])\n",
    "            image_T = image_T.cpu().detach().numpy().reshape(img_size, img_size)\n",
    "            image_T = np.array(list(map(round_function, image_T.flatten()))).reshape(128, 128)\n",
    "            image_T = centering_image(image_T, resize_fix=resize_fix)\n",
    "            fixed_target[idx] = torch.tensor(image_T).view([1, img_size, img_size])\n",
    "\n",
    "        fixed_label = np.array(font_ids)\n",
    "        source_with_label = [(label, image_S.cpu().detach().numpy()) \\\n",
    "                             for label, image_S in zip(fixed_label, fixed_source)]\n",
    "        source_with_label = sorted(source_with_label, key=lambda i: i[0])\n",
    "        target_with_label = [(label, image_T.cpu().detach().numpy()) \\\n",
    "                             for label, image_T in zip(fixed_label, fixed_target)]\n",
    "        target_with_label = sorted(target_with_label, key=lambda i: i[0])\n",
    "        fixed_source = torch.tensor(np.array([i[1] for i in source_with_label])).cuda()\n",
    "        fixed_target = torch.tensor(np.array([i[1] for i in target_with_label])).cuda()\n",
    "        fixed_label = sorted(fixed_label)\n",
    "        torch.save(fixed_source, os.path.join(save_dir, 'fixed_source.pkl'))\n",
    "        torch.save(fixed_target, os.path.join(save_dir, 'fixed_target.pkl'))\n",
    "        torch.save(fixed_label, os.path.join(save_dir, 'fixed_label.pkl'))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_dir = '../get_data/dataset-11172/'\n",
    "train_path = '../get_data/dataset_pkl/train.pickle'\n",
    "val_path = '../get_data/dataset_pkl/val.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_path, 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(val_path, 'rb') as f:\n",
    "    val = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../get_data/dataset-11172/\n",
      "26\n",
      "128\n",
      "102\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'extend'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-e2eed72f7862>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_batch_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrom_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwith_charid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-c1dd89d50782>\u001b[0m in \u001b[0;36mget_batch_iter\u001b[1;34m(examples, batch_size, augment, with_charid)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# the transpose ops requires deterministic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# batch size, thus comes the padding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mpadded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m           \u001b[1;31m## 배치사이즈를 맞게 만들어주는 함수\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m## 이미지의 테두리 일부분을 잘라서 세로로 합쳐서 저장해주는 함수\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-eedc09e77ff0>\u001b[0m in \u001b[0;36mpad_seq\u001b[1;34m(seq, batch_size)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mseq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mpadded\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mseq\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'extend'"
     ]
    }
   ],
   "source": [
    "get_batch_iter(from_dir, 128, True, with_charid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
